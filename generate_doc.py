# ------------------------------
# Exceptions spécifiques (pédagogie + robustesse)
# ------------------------------
from __future__ import (
    annotations,
)  # Permet les annotations de types en avant-référence (Python < 3.11)

import base64  # Encodage base64 requis pour envoyer des images à /api/generate
import json  # Sérialisation/désérialisation JSON
import shutil  # Pour trouver l'exécutable "ollama" dans le PATH
import socket  # Pour tester rapidement si un port TCP est ouvert
import subprocess  # Pour lancer "ollama serve"
import time  # Pour boucler avec un timeout lors du démarrage serveur
from dataclasses import dataclass  # Modèles de données simples et typés
from pathlib import Path  # Manipulation robuste des chemins
from typing import Any, Dict, List, Literal, Mapping, Optional, Sequence, Union

import numpy as np  # Types


# ------------------------------
# Exceptions spécifiques (pédagogie + robustesse)
# ------------------------------


class OllamaError(RuntimeError):
    """Erreur générique pour les opérations Ollama."""


class OllamaConnectionError(OllamaError):
    """Erreur de connexion au serveur Ollama."""


class OllamaResponseError(OllamaError):
    """Erreur lorsque la réponse HTTP/JSON d'Ollama est invalide ou inattendue."""


class OllamaServerStartError(OllamaError):
    """Erreur lorsque le serveur Ollama ne démarre pas correctement."""


# ------------------------------
# Dataclasses pour structurer les réponses (lisible + typé)
# ------------------------------


@dataclass(frozen=True, slots=True)
class OllamaModelDetails:
    """Détails d'un modèle, tels que renvoyés dans /api/tags."""

    format: Optional[str] = None
    family: Optional[str] = None
    families: Optional[List[str]] = None
    parameter_size: Optional[str] = None
    quantization_level: Optional[str] = None


@dataclass(frozen=True, slots=True)
class OllamaModelInfo:
    """Informations de base sur un modèle installé (issu de /api/tags)."""

    name: str
    modified_at: Optional[str] = None
    size: Optional[int] = None
    digest: Optional[str] = None
    details: Optional[OllamaModelDetails] = None


@dataclass(frozen=True, slots=True)
class OllamaGenerateResult:
    """Résultat simplifié de /api/generate en mode stream=false."""

    response: str
    model: Optional[str] = None
    done: Optional[bool] = None
    total_duration: Optional[int] = None  # ns (souvent)
    load_duration: Optional[int] = None
    prompt_eval_count: Optional[int] = None
    eval_count: Optional[int] = None


# ------------------------------
# Wrapper principal
# ------------------------------


class OllamaWrapper:
    """
    Wrapper Python (simple, robuste, typé) autour de l'API HTTP d'Ollama.

    Base URL par défaut : xxxx
    Endpoints utilisés :
      - GET  /api/version       (vérifier serveur)
      - GET  /api/tags          (liste des modèles installés)
      - POST /api/generate      (génération texte + multimodal via images base64)
    La doc officielle liste aussi d'autres endpoints (chat, pull, delete, etc.). :contentReference[oaicite:2]{index=2}
    """

    # Constructeur : on fixe l'hôte/timeout et on prépare des valeurs par défaut.
    def __init__(
        self,
        base_url: str = "http://10.22.28.190:11434",
        timeout_s: float = 600.0,
    ) -> None:
        self._base_url: str = base_url.rstrip("/")  # Normalise : pas de "/" final
        self._timeout_s: float = timeout_s  # Timeout réseau pour les requêtes HTTP

    # --------------------------
    # Méthodes "système"
    # --------------------------

    def is_server_running(self) -> bool:
        """
        Retourne True si le serveur Ollama répond.

        Stratégie : on essaie GET /api/version (léger et dédié à ça).
        """
        try:
            _ = self.get_version()  # Appel dédié à la version
            return True  # Si pas d'exception : serveur OK
        except OllamaConnectionError:
            return False  # Pas de serveur (ou port inaccessible)
        except OllamaResponseError:
            # Serveur répond mais réponse inattendue : on considère "running"
            return True

    def start_server(
        self,
        *,
        wait: bool = True,
        wait_timeout_s: float = 10.0,
        extra_env: Optional[Mapping[str, str]] = None,
    ) -> subprocess.Popen[bytes]:
        """
        Lance `ollama serve` via subprocess.

        Args:
            wait: si True, attend que le port réponde avant de rendre la main.
            wait_timeout_s: durée max d'attente si wait=True.
            extra_env: variables d'environnement supplémentaires (optionnel).

        Returns:
            Un objet Popen (process serveur).

        Raises:
            OllamaServerStartError: si ollama n'est pas trouvé ou ne démarre pas.
        """
        # Vérifie que l'exécutable "ollama" est présent dans le PATH.
        ollama_path: Optional[str] = shutil.which("ollama")
        if ollama_path is None:
            raise OllamaServerStartError(
                "Exécutable 'ollama' introuvable. Installe Ollama et/ou ajoute-le au PATH."
            )

        # Prépare l'environnement du processus.
        env: Dict[str, str] = dict(**(extra_env or {}))  # Copie défensive
        # Note : on laisse le reste de l'environnement hérité du parent (comportement standard).

        # Lance le serveur : stdout/stderr sont capturés pour debug/enseignement.
        process: subprocess.Popen[bytes] = subprocess.Popen(
            [ollama_path, "serve"],  # Commande
            stdout=subprocess.PIPE,  # Capture stdout
            stderr=subprocess.PIPE,  # Capture stderr
            env={**dict(**subprocess.os.environ), **env},  # Hérite + ajoute
        )

        # Si on ne veut pas attendre, on rend immédiatement la main.
        if not wait:
            return process

        # Attend que le serveur soit accessible, avec timeout.
        deadline: float = time.time() + wait_timeout_s
        while time.time() < deadline:
            if self._is_port_open():
                return process  # Serveur accessible : OK
            # Vérifie si le process est mort prématurément.
            if process.poll() is not None:
                # Process terminé -> on récupère stderr pour diagnostic.
                stderr: bytes = process.stderr.read() if process.stderr else b""
                raise OllamaServerStartError(
                    "Le serveur Ollama s'est arrêté pendant le démarrage.\n"
                    f"stderr:\n{stderr.decode(errors='replace')}"
                )
            time.sleep(0.1)  # Petite attente avant de retester

        # Timeout : serveur non accessible à temps.
        raise OllamaServerStartError(
            f"Le serveur Ollama ne répond pas après {wait_timeout_s:.1f}s."
        )

    def _is_port_open(self) -> bool:
        """
        Test rapide TCP du port du base_url (sans HTTP).

        Utile pour savoir si quelque chose écoute déjà, sans dépendre d'une réponse JSON.
        """
        host, port = self._parse_host_port()  # Extrait hôte/port de base_url
        try:
            with socket.create_connection((host, port), timeout=0.25):
                return True
        except OSError:
            return False

    def _parse_host_port(self) -> tuple[str, int]:
        """Parse très simple de base_url -> (host, port)."""
        # On gère uniquement le cas standard http://host:port
        # (pédagogiquement : clair ; production : on pourrait utiliser urllib.parse).
        url: str = self._base_url
        if url.startswith("http://"):
            url = url[len("http://") :]
        elif url.startswith("https://"):
            url = url[len("https://") :]
        # Si pas de port explicite, Ollama utilise 11434.
        if ":" in url:
            host, port_str = url.split(":", 1)
            return host, int(port_str)
        return url, 11434

    # --------------------------
    # Endpoints "information"
    # --------------------------

    def get_version(self) -> str:
        """
        Retourne la version du serveur Ollama via GET /api/version. :contentReference[oaicite:3]{index=3}
        """
        payload = self._http_request_json("GET", "/api/version", body=None)
        # La doc renvoie typiquement { "version": "x.y.z" }.
        version = payload.get("version")
        if not isinstance(version, str):
            raise OllamaResponseError(f"Réponse /api/version inattendue: {payload!r}")
        return version

    def list_models(self) -> List[OllamaModelInfo]:
        """
        Liste les modèles installés via GET /api/tags. :contentReference[oaicite:4]{index=4}
        """
        payload = self._http_request_json("GET", "/api/tags", body=None)
        raw_models = payload.get("models")
        if not isinstance(raw_models, list):
            raise OllamaResponseError(f"Réponse /api/tags inattendue: {payload!r}")

        models: List[OllamaModelInfo] = []
        for item in raw_models:
            # Chaque entrée doit être un dict.
            if not isinstance(item, dict):
                continue

            name = item.get("name")
            if not isinstance(name, str):
                continue

            # Détails optionnels.
            raw_details = item.get("details")
            details: Optional[OllamaModelDetails] = None
            if isinstance(raw_details, dict):
                details = OllamaModelDetails(
                    format=(
                        raw_details.get("format")
                        if isinstance(raw_details.get("format"), str)
                        else None
                    ),
                    family=(
                        raw_details.get("family")
                        if isinstance(raw_details.get("family"), str)
                        else None
                    ),
                    families=(
                        raw_details.get("families")
                        if isinstance(raw_details.get("families"), list)
                        else None
                    ),
                    parameter_size=(
                        raw_details.get("parameter_size")
                        if isinstance(raw_details.get("parameter_size"), str)
                        else None
                    ),
                    quantization_level=(
                        raw_details.get("quantization_level")
                        if isinstance(raw_details.get("quantization_level"), str)
                        else None
                    ),
                )

            models.append(
                OllamaModelInfo(
                    name=name,
                    modified_at=(
                        item.get("modified_at")
                        if isinstance(item.get("modified_at"), str)
                        else None
                    ),
                    size=(
                        item.get("size") if isinstance(item.get("size"), int) else None
                    ),
                    digest=(
                        item.get("digest")
                        if isinstance(item.get("digest"), str)
                        else None
                    ),
                    details=details,
                )
            )

        return models

    # --------------------------
    # Génération
    # --------------------------

    def generate_text(
        self,
        *,
        model: str,
        prompt: str,
        system: Optional[str] = None,
        options: Optional[Mapping[str, Any]] = None,
    ) -> OllamaGenerateResult:
        """
        Appelle POST /api/generate en texte seul (stream=false). :contentReference[oaicite:5]{index=5}

        Args:
            model: nom du modèle (ex: "llama3", "mistral", etc.)
            prompt: le prompt utilisateur
            system: message système (optionnel, dépend des modèles/versions)
            options: paramètres avancés Ollama (température, top_p, seed, etc.)

        Returns:
            OllamaGenerateResult : réponse texte + quelques métriques si présentes.
        """
        body: Dict[str, Any] = {
            "model": model,  # Modèle ciblé
            "prompt": prompt,  # Prompt texte
            "stream": False,  # On veut une réponse complète en une fois
        }

        # Ajoute le système si fourni.
        if system is not None:
            body["system"] = system

        # Ajoute les options avancées si fournies.
        if options is not None:
            body["options"] = dict(options)

        payload = self._http_request_json("POST", "/api/generate", body=body)

        # Extrait la réponse principale.
        response_text = payload.get("response")
        if not isinstance(response_text, str):
            raise OllamaResponseError(f"Réponse /api/generate inattendue: {payload!r}")

        return OllamaGenerateResult(
            response=response_text,
            model=(
                payload.get("model") if isinstance(payload.get("model"), str) else None
            ),
            done=payload.get("done") if isinstance(payload.get("done"), bool) else None,
            total_duration=(
                payload.get("total_duration")
                if isinstance(payload.get("total_duration"), int)
                else None
            ),
            load_duration=(
                payload.get("load_duration")
                if isinstance(payload.get("load_duration"), int)
                else None
            ),
            prompt_eval_count=(
                payload.get("prompt_eval_count")
                if isinstance(payload.get("prompt_eval_count"), int)
                else None
            ),
            eval_count=(
                payload.get("eval_count")
                if isinstance(payload.get("eval_count"), int)
                else None
            ),
        )

    # --------------------------
    # Bonus : embeddings (optionnel pour ton cours)
    # --------------------------

    def embed(
        self,
        *,
        model: str,
        text: str,
    ) -> List[float]:
        """
        Génère un embedding.

        Note doc : l'endpoint "Generate Embedding" a été supersédé par /api/embed
        (selon docs/api.md). :contentReference[oaicite:7]{index=7}
        """
        body: Dict[str, Any] = {"model": model, "input": text}
        payload = self._http_request_json("POST", "/api/embed", body=body)

        # Selon versions, la forme peut varier ; on vise un cas courant : {"embeddings":[[...]]} ou {"embedding":[...]}.
        if isinstance(payload.get("embedding"), list):
            emb = payload["embedding"]
            if all(isinstance(x, (int, float)) for x in emb):
                return [float(x) for x in emb]

        if isinstance(payload.get("embeddings"), list) and payload["embeddings"]:
            first = payload["embeddings"][0]
            if isinstance(first, list) and all(
                isinstance(x, (int, float)) for x in first
            ):
                return [float(x) for x in first]

        raise OllamaResponseError(f"Réponse /api/embed inattendue: {payload!r}")

    # --------------------------
    # Couche HTTP (sans dépendance externe)
    # --------------------------

    def _http_request_json(
        self,
        method: str,
        path: str,
        *,
        body: Optional[Mapping[str, Any]],
    ) -> Dict[str, Any]:
        """
        Exécute une requête HTTP et retourne un dict JSON.

        On utilise urllib (stdlib) pour éviter une dépendance à requests/httpx dans un contexte étudiant.
        """
        # Import local pour ne pas polluer le namespace global et montrer le principe.
        import urllib.error  # Exceptions réseau HTTP
        import urllib.request  # Client HTTP standard
        from urllib.parse import urljoin  # Construit proprement l'URL finale

        # Construit l'URL complète.
        url: str = urljoin(self._base_url + "/", path.lstrip("/"))

        # Prépare les headers.
        headers: Dict[str, str] = {
            "Accept": "application/json",  # On attend du JSON
            "Content-Type": "application/json",  # Si body présent
        }

        # Sérialise le body en JSON si nécessaire.
        data: Optional[bytes]
        if body is None:
            data = None
        else:
            data = json.dumps(body).encode("utf-8")  # JSON -> bytes UTF-8

        # Construit l'objet Request.
        request = urllib.request.Request(
            url=url,
            data=data,
            headers=headers,
            method=method.upper(),
        )

        try:
            # Exécute la requête avec timeout.
            with urllib.request.urlopen(request, timeout=self._timeout_s) as response:
                raw: bytes = response.read()  # Lit tout le corps
        except urllib.error.URLError as e:
            # Typiquement : connection refused, host unreachable, timeout, etc.
            raise OllamaConnectionError(
                f"Impossible de joindre Ollama à {url}: {e}"
            ) from e
        except Exception as e:
            # Autres erreurs réseau inattendues.
            raise OllamaConnectionError(f"Erreur réseau vers {url}: {e}") from e

        # Décode en texte.
        text = raw.decode("utf-8", errors="replace")

        # Parse JSON.
        try:
            payload = json.loads(text)
        except json.JSONDecodeError as e:
            raise OllamaResponseError(
                f"Réponse non-JSON depuis {url} (début): {text[:200]!r}"
            ) from e

        # On attend un dict JSON.
        if not isinstance(payload, dict):
            raise OllamaResponseError(f"JSON inattendu depuis {url}: {payload!r}")

        return payload


ollama = OllamaWrapper(base_url="http://10.22.28.190:11434")
MODEL_NAME = "gemma2:latest"
SYSTEM_PROMPT = (
    "Tu es un assistant qui ajoute des commentaires et docstrings à toutes les fonctions et constructeurs "
    "dans le code fourni, sans modifier le code existant."
)

# ------------------------------
# Configuration du projet
# ------------------------------
SOURCE_DIRS = ["Arcade"]
EXTENSIONS = [".py", ".java", ".lua"]

# ------------------------------
# Fonction principale
# ------------------------------
def annotate_file(file_path: Path):
    print(f"➡️ Traitement de {file_path}")
    code = file_path.read_text(encoding="utf-8")
    
    prompt = f"Ajoute des commentaires/docstrings au code suivant sans modifier le code existant. Ne renvoit que le texte du code en brut, sans formatage MARKDOWN ou texte descriptif autre que du code.\n\n{code}\n\n N'inclu pas de formatage de code dans ta réponse du style ```, la réponse doit être le contenu du fichier executable"
    
    try:
        result = ollama.generate_text(model=MODEL_NAME, prompt=prompt, system=SYSTEM_PROMPT)
        annotated_code = result.response
        file_path.write_text(annotated_code, encoding="utf-8")
        print(f"✅ Code annoté écrit dans : {file_path}")
    except Exception as e:
        print(f"⚠️ Erreur lors de l'annotation de {file_path}: {e}")

def main():
    for src_dir in SOURCE_DIRS:
        path = Path(src_dir)
        if not path.exists():
            print(f"⚠️ Dossier inexistant : {src_dir}")
            continue
        for file_path in path.rglob("*"):
            if file_path.suffix in EXTENSIONS:
                annotate_file(file_path)

if __name__ == "__main__":
    main()
